{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ../models/Hermes-2-Pro-Llama-3-8B-Q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Hermes-2-Pro-Llama-3-8B\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128288\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128288]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128288]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128003\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{bos_token}}{% for message in messag...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 288\n",
      "llm_load_vocab: token to piece cache size = 0.8007 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128288\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Hermes-2-Pro-Llama-3-8B\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128003 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128003 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.27 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.88 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4403.59 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   258.56 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{bos_token}}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128003', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'Hermes-2-Pro-Llama-3-8B', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '15', 'llama.vocab_size': '128288', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "import instructor\n",
    "\n",
    "#from llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "llama = llama_cpp.Llama(\n",
    "    model_path=\"../models/Hermes-2-Pro-Llama-3-8B-Q4.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=2048,\n",
    "    chat_format=\"llama-3\",\n",
    ")\n",
    "\n",
    "\n",
    "create = instructor.patch(\n",
    "    create=llama.create_chat_completion_openai_v1,\n",
    "    mode=instructor.Mode.JSON_SCHEMA, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionExtraction(BaseModel):\n",
    "    chain_of_thought: str = Field(\n",
    "        ...,\n",
    "        description=\"The chain of thought that led to the prediction.\",\n",
    "    )\n",
    "    user_input_code: str = Field(\n",
    "        ...,\n",
    "        description=\"Code that the user will implement, such as blank function implementations for the user to implement. Use the question prompt to help determine what the user needs to implement.\",\n",
    "    )\n",
    "    predefined_code: str = Field(\n",
    "        ...,\n",
    "        description=\"Code that is predefined by the system, such as imports, function definitions, struct definitions, etc.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_question_data(data: str) -> QuestionExtraction:\n",
    "    extraction: QuestionExtraction = create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an expert at extracting information from exams. \"\n",
    "                    \"You will be given a question from a Computer Science exam, \"\n",
    "                    \"and you will need to extract metadata about the question. \"\n",
    "                    \"Focus on identifying the user_input_code and predefined_code. \"\n",
    "                    \"Do not solve the question. \"\n",
    "                    \"For example, if the question provides a function prototype, \"\n",
    "                    \"extract it as user_input_code. If there are any predefined \"\n",
    "                    \"function implementations or imports, extract them as predefined_code.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Extract user_input_code and predefined_code from the following text: \"\n",
    "                    f\"<text>{data}</text>\"\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "        response_model=QuestionExtraction,\n",
    "    )\n",
    "    return extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A mininum  heap  is typically implemented with an array, with the root node ( minimum value ) being \n",
      "stored in index 1 of the array. To insert a new value into a heap, it’s originally placed in the first open slot, \n",
      "followed by running a “percolate up” operation. Write a function that inserts a value into a heap in this \n",
      "manner. You may assume that the array is allocated to be big enough to store the new ly inserted value. \n",
      "The function prototype is as follows:  \n",
      " \n",
      "void insert(int* heap, int curSize, int newVal);  \n",
      " \n",
      "heap is a pointer to an array which currently stores curSize  number of values (but has room for at \n",
      "least 1 more). newVal  is the new number to be ins erted into the heap. Write this function which inserts \n",
      "the value newVal into this minimum heap . Take care to avoid infinite loops or array out of bounds issues. \n",
      "You may assume that index curSize+1 is in bounds for the array heap. Also, remember that index 0 of the \n",
      "array heap is unused.  You may not write any helper functions.  \n",
      " \n",
      "void insert(int* heap, int curSize, int newVal) {  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "}\n",
      "chain-of-thought-kv ::= [\"] [c] [h] [a] [i] [n] [_] [o] [f] [_] [t] [h] [o] [u] [g] [h] [t] [\"] space [:] space string \n",
      "space ::= space_8 \n",
      "string ::= [\"] string_9 [\"] space \n",
      "char ::= [^\"\\] | [\\] char_4 \n",
      "char_4 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "predefined-code-kv ::= [\"] [p] [r] [e] [d] [e] [f] [i] [n] [e] [d] [_] [c] [o] [d] [e] [\"] space [:] space string \n",
      "root ::= [{] space chain-of-thought-kv [,] space user-input-code-kv [,] space predefined-code-kv [}] space \n",
      "user-input-code-kv ::= [\"] [u] [s] [e] [r] [_] [i] [n] [p] [u] [t] [_] [c] [o] [d] [e] [\"] space [:] space string \n",
      "space_8 ::= [ ] | \n",
      "string_9 ::= char string_9 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     349.65 ms\n",
      "llama_print_timings:      sample time =    1141.86 ms /   104 runs   (   10.98 ms per token,    91.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     434.01 ms /   635 tokens (    0.68 ms per token,  1463.08 tokens per second)\n",
      "llama_print_timings:        eval time =    1995.69 ms /   103 runs   (   19.38 ms per token,    51.61 tokens per second)\n",
      "llama_print_timings:       total time =    3934.56 ms /   738 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"chain_of_thought\": \"The user needs to implement a function that inserts a new value into a minimum heap using an array. The function prototype is provided, and the user should use the prompt to understand what to implement.\",\n",
      "  \"user_input_code\": \"void insert(int* heap, int curSize, int newVal);\",\n",
      "  \"predefined_code\": \"void insert(int* heap, int curSize, int newVal) {\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from parser.model import Section\n",
    "from parser.parse import main\n",
    "\n",
    "sections: List[Section] = main(\"../fe_files/exams/FE-Aug23.pdf\")\n",
    "\n",
    "input_question = sections[1].questions[1].text;\n",
    "\n",
    "print(input_question)\n",
    "\n",
    "extraction = extract_question_data(input_question)\n",
    "\n",
    "print(extraction.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parser-2Aur3UYS-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
