{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import things that are needed generically\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import ToolNode\n",
    "# from langchain_openai.chat_models.base import _convert_message_to_dict\n",
    "from langchain_core.messages import SystemMessage\n",
    "from typing import Optional\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"a detailed sentence for efficient search\")\n",
    "\n",
    "class WeatherInput(BaseModel):\n",
    "    location: str = Field(description=\"The city and state, e.g. San Francisco, CA\")\n",
    "    unit: str = Field(enum=[\"celsius\", \"fahrenheit\"])\n",
    "\n",
    "@tool(\"search_online\", args_schema=SearchInput)\n",
    "def search(query: str):\n",
    "    \"\"\"An online search tool to retrieve the most accurate and up-to-date, lastest information available.\"\"\"\n",
    "\n",
    "    tool = TavilySearchResults(max_results=2)\n",
    "    results = tool.invoke(query) \n",
    "    content = '\\n\\n'.join([f\"{index+1}. {i['content']}\" for index, i in enumerate(results)])\n",
    "\n",
    "    return content\n",
    "\n",
    "@tool(\"get_current_weather\", args_schema=WeatherInput)\n",
    "def get_weather(location: str, unit:str):\n",
    "    \"\"\"This function retrieves the latest weather information for a specified location. \"\"\"\n",
    "\n",
    "    return f\"Now the weather in {location} is 22 {unit}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59448/1261768887.py:3: LangGraphDeprecationWarning: ToolExecutor is deprecated as of version 0.2.0 and will be removed in 0.3.0. Use langgraph.prebuilt.ToolNode instead.\n",
      "  tool_executor = ToolExecutor([search, get_weather])\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "tool_executor = ToolExecutor([search, get_weather])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llamacpp_function_calling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_stdout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingStdOutCallbackHandler\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatLlamaCpp\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllamacpp_function_calling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_format\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hermes_pro_function_calling\n\u001b[1;32m      7\u001b[0m local_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../models/Hermes-2-Pro-Llama-3-8B-Q8.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatLlamaCpp(\n\u001b[1;32m     10\u001b[0m     temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     11\u001b[0m     model_path \u001b[38;5;241m=\u001b[39mlocal_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     chat_handler\u001b[38;5;241m=\u001b[39mhermes_pro_function_calling\n\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llamacpp_function_calling'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import multiprocessing\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.chat_models import ChatLlamaCpp\n",
    "from llamacpp_function_calling.chat_format import hermes_pro_function_calling\n",
    "\n",
    "local_model = \"../models/Hermes-2-Pro-Llama-3-8B-Q8.gguf\"\n",
    "\n",
    "model = ChatLlamaCpp(\n",
    "    temperature = 0.5,\n",
    "    model_path =local_model,\n",
    "    n_ctx = 10000,\n",
    "    n_gpu_layers = 20,\n",
    "    n_batch = 850,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_threads = multiprocessing.cpu_count()-5,\n",
    "    # callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]),# Callbacks support token-wise streaming\n",
    "    max_tokens = 512,\n",
    "    verbose= True,\n",
    "    chat_handler=hermes_pro_function_calling\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parser-2Aur3UYS-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
