{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leet-Code Style Question Title Generation AI Model - Test 1:\n",
    "- **Model**: `google/flan-t5-large`\n",
    "- **Goal**: Utilize a <u>pretrained model</u> to generate concise, descriptive titles, for questions from UCF's Foundation Exam <u>based solely on provided question input</u> (not the solution) while <u>mimicing LeetCode's style and syntax</u> for question titles.\n",
    "- **Input**: <u>Detailed question statements</u> from UCF's Foundation Exams contained in the `formatted_questions.json` file.  <br>\n",
    "\n",
    "  For the purpose of this test, the `formatted_questions.json` file contains only 12 Foundation Exam questions (parsed from the \n",
    "  Fall 2023 Foundation Exam) with associated ids in the following format:  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;{  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"id\" : 1,  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"content\" : \"Example question here\"  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;}  <br>   \n",
    "\n",
    "  The script `format_questions.py` uses the parsed Foundation Exam content generated from `parser/parse.py` stored in `document.json` under the `text:` tag to structure the formatted_questions.json file in the necessary format for the `google/flan-t5-large` model to properly comprehend and subsequently run inference on.  <br>\n",
    "\n",
    "- **Output**: <u>Descriptive LeetCode-style question titles</u> with their associated ids saved to `question_titles.json` in the following \n",
    "  format:  <br>\n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;{  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"id\" : 1,  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"generated_title\" : \"Generated Question Title Here.\"  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;}  <br>\n",
    "\n",
    "- **Finetuning**: <u>None</u>, the goal of this test is to determine if the `google/flan-t5-large` model sufficiently achieves the above\n",
    "  referenced goal without any finetuning. For details and explanation of why the `google/flan-t5-model` was selected as opposed to other sequence-to-sequence transformer models, refer to the Generative Artifical Intelligence for Natural Language Processing section \n",
    "  of the Design Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
